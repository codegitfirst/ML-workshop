{
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Sarcasm Detection Using Classical ML Techniques",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 533474,
          "sourceType": "datasetVersion",
          "datasetId": 30764
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codegitfirst/ML-workshop/blob/main/Sarcasm_Detection_Using_Classical_ML_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'news-headlines-dataset-for-sarcasm-detection:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F30764%2F533474%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240729%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240729T080417Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5557b96d4a66a0609dfa761c9385e58a38de2b52efad5180ef3986094908415c178163f5f1165944e0558706e4b8b81470b2a51769ac00938100a05371a9f931dae450c2e8604053236630c33750862f15c4618732fe2b113ad0a5d260628157a0e0dfc091c0beb2704e6990cb980be57bf61a177453052738ae0ea81ee2f1542b3634aeeb3bccf1111653774a428c8e51d8df91cc973f89f345cc5fbebcdac629afb0784f2f9c6c537aa58de78c24e4a4aa8931bf59de8f0787149766de41f78bdd38bce5d2bcc3f3ef189b79e218049427fa1d60a50b9966b000d4d67f21493222c372b0474c7aad24c65135bd2ba3b382bbe2abeb19c15ff396e1b57f36c8'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "tccYcbNUVsI8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sarcasm Detection Using Classical ML Techniques\n",
        "\n",
        "data : https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection"
      ],
      "metadata": {
        "id": "Xq10nTM_KH5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading and exploring the data :"
      ],
      "metadata": {
        "id": "mmUFa7WPKkfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "FtubMBXxXT5T",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:02.266468Z",
          "iopub.execute_input": "2024-06-10T08:25:02.266999Z",
          "iopub.status.idle": "2024-06-10T08:25:05.182067Z",
          "shell.execute_reply.started": "2024-06-10T08:25:02.266954Z",
          "shell.execute_reply": "2024-06-10T08:25:05.180906Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json('/kaggle/input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset_v2.json',lines=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2WIJVssQdJVb",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:05.184295Z",
          "iopub.execute_input": "2024-06-10T08:25:05.184776Z",
          "iopub.status.idle": "2024-06-10T08:25:05.500487Z",
          "shell.execute_reply.started": "2024-06-10T08:25:05.184745Z",
          "shell.execute_reply": "2024-06-10T08:25:05.499302Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "hNx8UJFveLpe",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:05.501996Z",
          "iopub.execute_input": "2024-06-10T08:25:05.50241Z",
          "iopub.status.idle": "2024-06-10T08:25:05.534677Z",
          "shell.execute_reply.started": "2024-06-10T08:25:05.502371Z",
          "shell.execute_reply": "2024-06-10T08:25:05.533415Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "KaQ7x-j8euwO",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:05.537817Z",
          "iopub.execute_input": "2024-06-10T08:25:05.538294Z",
          "iopub.status.idle": "2024-06-10T08:25:05.550721Z",
          "shell.execute_reply.started": "2024-06-10T08:25:05.538255Z",
          "shell.execute_reply": "2024-06-10T08:25:05.549439Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "BJQxf3-2e282",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:05.552394Z",
          "iopub.execute_input": "2024-06-10T08:25:05.552775Z",
          "iopub.status.idle": "2024-06-10T08:25:05.571808Z",
          "shell.execute_reply.started": "2024-06-10T08:25:05.552745Z",
          "shell.execute_reply": "2024-06-10T08:25:05.570529Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of duplicate headlines in the 'headline' column\n",
        "df.headline.duplicated().sum()"
      ],
      "metadata": {
        "id": "Ju-rHwG-e_DC",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:05.57396Z",
          "iopub.execute_input": "2024-06-10T08:25:05.574403Z",
          "iopub.status.idle": "2024-06-10T08:25:05.593226Z",
          "shell.execute_reply.started": "2024-06-10T08:25:05.574364Z",
          "shell.execute_reply": "2024-06-10T08:25:05.592086Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with duplicate headlines from the DataFrame\n",
        "df = df.drop(df[df.headline.duplicated()].index, axis=0)"
      ],
      "metadata": {
        "id": "s7e_ug9tfOHd",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:05.595308Z",
          "iopub.execute_input": "2024-06-10T08:25:05.595737Z",
          "iopub.status.idle": "2024-06-10T08:25:05.617397Z",
          "shell.execute_reply.started": "2024-06-10T08:25:05.5957Z",
          "shell.execute_reply": "2024-06-10T08:25:05.615808Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a bar chart of the counts of sarcastic and non-sarcastic labels\n",
        "value_counts = df.is_sarcastic.value_counts()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=value_counts.index, y=value_counts.values, palette='viridis')\n",
        "plt.title('Counts of Sarcastic and Non-Sarcastic Headlines')\n",
        "plt.xlabel('Label (0 = Non-Sarcastic, 1 = Sarcastic)')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['Non-Sarcastic', 'Sarcastic'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9rZ3Y4gIiE00",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:05.619031Z",
          "iopub.execute_input": "2024-06-10T08:25:05.619463Z",
          "iopub.status.idle": "2024-06-10T08:25:05.9243Z",
          "shell.execute_reply.started": "2024-06-10T08:25:05.619419Z",
          "shell.execute_reply": "2024-06-10T08:25:05.92292Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "vdT3e6N5KzqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the stopwords and punkt tokenizer from NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Get the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def data_preprocessing(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphabetic characters and replace them with spaces\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    text = word_tokenize(text)\n",
        "\n",
        "    # Lemmatize each word and remove stopwords\n",
        "    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n",
        "\n",
        "    # Join the processed words back into a single string\n",
        "    return ' '.join(text)\n"
      ],
      "metadata": {
        "id": "9oEqWvXWi2nm",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:05.925941Z",
          "iopub.execute_input": "2024-06-10T08:25:05.926373Z",
          "iopub.status.idle": "2024-06-10T08:25:07.075604Z",
          "shell.execute_reply.started": "2024-06-10T08:25:05.92634Z",
          "shell.execute_reply": "2024-06-10T08:25:07.073786Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "qV8OozKnK5f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the feature set (headlines) and the target variable (sarcasm labels)\n",
        "X = df.headline\n",
        "y = df.is_sarcastic\n",
        "\n",
        "# Split the data into training and testing sets with 30% of the data for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "XnTqxKNc09yW",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:07.079681Z",
          "iopub.execute_input": "2024-06-10T08:25:07.080117Z",
          "iopub.status.idle": "2024-06-10T08:25:07.097494Z",
          "shell.execute_reply.started": "2024-06-10T08:25:07.080084Z",
          "shell.execute_reply": "2024-06-10T08:25:07.095992Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "# Function to vectorize the training and testing data using TF-IDF\n",
        "def vectorize_data(X_train, X_test):\n",
        "    vectorizer = TfidfVectorizer()  # Initialize TF-IDF vectorizer with a max feature limit\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)  # Fit and transform the training data\n",
        "    X_test_tfidf = vectorizer.transform(X_test)  # Transform the testing data\n",
        "    return X_train_tfidf, X_test_tfidf\n",
        "\n",
        "# Function to train a model and evaluate its performance\n",
        "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)  # Train the model on the training data\n",
        "    y_pred = model.predict(X_test)  # Predict the labels for the testing data\n",
        "    accuracy = accuracy_score(y_test, y_pred)  # Calculate the accuracy of the model\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)  # Generate the confusion matrix\n",
        "    return accuracy, conf_matrix\n",
        "\n",
        "# Main function to vectorize data, train models, and display results\n",
        "def main(X_train, y_train, X_test, y_test):\n",
        "    X_train_tfidf, X_test_tfidf = vectorize_data(X_train, X_test)  # Vectorize the data\n",
        "\n",
        "    results = []  # List to store results\n",
        "    confusion_matrices = {}  # Dictionary to store confusion matrices\n",
        "\n",
        "    # Logistic Regression\n",
        "    lr_model = LogisticRegression()  # Initialize Logistic Regression model\n",
        "    accuracy_lr, conf_matrix_lr = train_and_evaluate_model(lr_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
        "    results.append(['Logistic Regression', accuracy_lr])  # Append accuracy to results\n",
        "    confusion_matrices['Logistic Regression'] = conf_matrix_lr  # Store confusion matrix\n",
        "\n",
        "    # Support Vector Machine\n",
        "    svm_model = SVC(kernel='linear')  # Initialize SVM model with linear kernel\n",
        "    accuracy_svm, conf_matrix_svm = train_and_evaluate_model(svm_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
        "    results.append(['Support Vector Machine', accuracy_svm])  # Append accuracy to results\n",
        "    confusion_matrices['Support Vector Machine'] = conf_matrix_svm  # Store confusion matrix\n",
        "\n",
        "    # Multinomial Naive Bayes\n",
        "    nb_model = MultinomialNB()  # Initialize Multinomial Naive Bayes model\n",
        "    accuracy_nb, conf_matrix_nb = train_and_evaluate_model(nb_model, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
        "    results.append(['Multinomial Naive Bayes', accuracy_nb])  # Append accuracy to results\n",
        "    confusion_matrices['Multinomial Naive Bayes'] = conf_matrix_nb  # Store confusion matrix\n",
        "\n",
        "    # Display the results in a table\n",
        "    results_df = pd.DataFrame(results, columns=['Model', 'Accuracy'])\n",
        "    print(results_df)\n",
        "\n",
        "    return confusion_matrices\n",
        "\n",
        "confusion_matrices = main(X_train, y_train, X_test, y_test)  # Call the main function and store confusion matrices"
      ],
      "metadata": {
        "id": "482PlcADIPol",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:25:07.099428Z",
          "iopub.execute_input": "2024-06-10T08:25:07.099848Z",
          "iopub.status.idle": "2024-06-10T08:26:29.015477Z",
          "shell.execute_reply.started": "2024-06-10T08:25:07.099817Z",
          "shell.execute_reply": "2024-06-10T08:26:29.014179Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to plot confusion matrices for each model\n",
        "def plot_confusion_matrices(confusion_matrices):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))  # Create a 2x2 grid of subplots\n",
        "    model_names = list(confusion_matrices.keys())  # Get the model names from the dictionary keys\n",
        "\n",
        "    # Iterate over the axes and model names\n",
        "    for ax, model_name in zip(axes.flat, model_names):\n",
        "        # Plot the confusion matrix as a heatmap for each model\n",
        "        sns.heatmap(confusion_matrices[model_name], annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "        ax.set_title(f'Confusion Matrix for {model_name}')  # Set the title for each subplot\n",
        "        ax.set_xlabel('Predicted')  # Set the x-axis label\n",
        "        ax.set_ylabel('Actual')  # Set the y-axis label\n",
        "\n",
        "    # Hide any empty subplots if there are fewer than 4 models\n",
        "    for ax in axes.flat[len(model_names):]:\n",
        "        ax.set_visible(False)\n",
        "\n",
        "    plt.tight_layout()  # Adjust the layout to prevent overlap\n",
        "    plt.show()  # Display the plot\n",
        "\n",
        "plot_confusion_matrices(confusion_matrices)  # Call the function to plot confusion matrices\n"
      ],
      "metadata": {
        "id": "mm1or6PuIUQL",
        "execution": {
          "iopub.status.busy": "2024-06-10T08:26:29.017009Z",
          "iopub.execute_input": "2024-06-10T08:26:29.017374Z",
          "iopub.status.idle": "2024-06-10T08:26:30.323558Z",
          "shell.execute_reply.started": "2024-06-10T08:26:29.017334Z",
          "shell.execute_reply": "2024-06-10T08:26:30.322298Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQAe7R09IppA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}